{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Financial News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example of LDA applied to financial news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:17.655079Z",
     "start_time": "2024-05-31T17:22:17.652488Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:19.154638Z",
     "start_time": "2024-05-31T17:22:17.660676Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# spacy for language processing\n",
    "import spacy\n",
    "\n",
    "# sklearn for feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# gensim for topic models\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "# topic model viz\n",
    "import pyLDAvis\n",
    "from pyLDAvis.gensim_models import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:19.159990Z",
     "start_time": "2024-05-31T17:22:19.156456Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:32.307368Z",
     "start_time": "2024-05-31T17:22:31.706008Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(pd.read_csv('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words',\n",
    "                             header=None).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Viz Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.575633Z",
     "start_time": "2024-05-31T17:22:35.570720Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_word_list(model, corpus, top=10, save=False):\n",
    "    top_topics = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    words, probs = [], []\n",
    "    for top_topic, _ in top_topics:\n",
    "        words.append([t[1] for t in top_topic[:top]])\n",
    "        probs.append([t[0] for t in top_topic[:top]])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(model.num_topics*1.2, 5))\n",
    "    sns.heatmap(pd.DataFrame(probs).T,\n",
    "                annot=pd.DataFrame(words).T,\n",
    "                fmt='',\n",
    "                ax=ax,\n",
    "                cmap='Blues',\n",
    "                cbar=False)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(f'fin_news_wordlist_{top}', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.702677Z",
     "start_time": "2024-05-31T17:22:35.696459Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_coherence(model, corpus, tokens, top=10, cutoff=0.01):\n",
    "    top_topics = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    word_lists = pd.DataFrame(model.get_topics().T, index=tokens)\n",
    "    order = []\n",
    "    for w, word_list in word_lists.items():\n",
    "        target = set(word_list.nlargest(top).index)\n",
    "        for t, (top_topic, _) in enumerate(top_topics):\n",
    "            if target == set([t[1] for t in top_topic[:top]]):\n",
    "                order.append(t)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(15,5))\n",
    "    title = f'# Words with Probability > {cutoff:.2%}'\n",
    "    (word_lists.loc[:, order]>cutoff).sum().reset_index(drop=True).plot.bar(title=title, ax=axes[1]);\n",
    "\n",
    "    umass = model.top_topics(corpus=corpus, coherence='u_mass', topn=20)\n",
    "    pd.Series([c[1] for c in umass]).plot.bar(title='Topic Coherence', ax=axes[0])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'fin_news_coherence_{top}', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.723096Z",
     "start_time": "2024-05-31T17:22:35.718450Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_top_docs(model, corpus, docs):\n",
    "    doc_topics = model.get_document_topics(corpus)\n",
    "    df = pd.concat([pd.DataFrame(doc_topic, \n",
    "                                 columns=['topicid', 'weight']).assign(doc=i) \n",
    "                    for i, doc_topic in enumerate(doc_topics)])\n",
    "\n",
    "    for topicid, data in df.groupby('topicid'):\n",
    "        print(topicid, docs[int(data.sort_values('weight', ascending=False).iloc[0].doc)])\n",
    "        print(pd.DataFrame(lda.show_topic(topicid=topicid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load Financial News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is avaialble from [Kaggle](https://www.kaggle.com/jeet2016/us-financial-news-articles). \n",
    "\n",
    "Download and unzip into data directory in repository root folder, then rename the enclosing folder to `us-financial-news` and the subfolders so you get the following directory structure:\n",
    "```\n",
    "data\n",
    "  |-us-financial-news\n",
    "     |-2018_01\n",
    "     |-2018_02\n",
    "     |-2018_03\n",
    "     |-2018_04\n",
    "     |-2018_05\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.734321Z",
     "start_time": "2024-05-31T17:22:35.731345Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = Path('..', 'data', 'us-financial-news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the article selection to the following sections in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.751071Z",
     "start_time": "2024-05-31T17:22:35.748219Z"
    }
   },
   "outputs": [],
   "source": [
    "section_titles = ['Press Releases - CNBC',\n",
    "                  'Reuters: Company News',\n",
    "                  'Reuters: World News',\n",
    "                  'Reuters: Business News',\n",
    "                  'Reuters: Financial Services and Real Estate',\n",
    "                  'Top News and Analysis (pro)',\n",
    "                  'Reuters: Top News',\n",
    "                  'The Wall Street Journal &amp; Breaking News, Business, Financial and Economic News, World News and Video',\n",
    "                  'Business &amp; Financial News, U.S &amp; International Breaking News | Reuters',\n",
    "                  'Reuters: Money News',\n",
    "                  'Reuters: Technology News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.772125Z",
     "start_time": "2024-05-31T17:22:35.768432Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_articles():\n",
    "    articles = []\n",
    "    counter = Counter()\n",
    "    for f in data_path.glob('*/**/*.json'):\n",
    "        article = json.load(f.open())\n",
    "        if article['thread']['section_title'] in set(section_titles):\n",
    "            text = article['text'].lower().split()\n",
    "            counter.update(text)\n",
    "            articles.append(' '.join([t for t in text if t not in stop_words]))\n",
    "    return articles, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.796736Z",
     "start_time": "2024-05-31T17:22:35.792864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading 0 articles\n"
     ]
    }
   ],
   "source": [
    "articles, counter = read_articles()\n",
    "print(f'Done loading {len(articles):,.0f} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.811697Z",
     "start_time": "2024-05-31T17:22:35.806817Z"
    }
   },
   "outputs": [],
   "source": [
    "most_common = (pd.DataFrame(counter.most_common(), columns=['token', 'count'])\n",
    "               .pipe(lambda x: x[~x.token.str.lower().isin(stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.826434Z",
     "start_time": "2024-05-31T17:22:35.819152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [token, count]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.846835Z",
     "start_time": "2024-05-31T17:22:35.843689Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path('results', 'financial_news')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:35.871476Z",
     "start_time": "2024-05-31T17:22:35.868048Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_doc(d):\n",
    "    doc = []\n",
    "    for t in d:\n",
    "        if not any([t.is_stop, t.is_digit, not t.is_alpha, t.is_punct, t.is_space, t.lemma_ == '-PRON-']):        \n",
    "            doc.append(t.lemma_)\n",
    "    return ' '.join(doc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:36.137704Z",
     "start_time": "2024-05-31T17:22:35.895698Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models and if you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m nlp\u001B[38;5;241m.\u001B[39mmax_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m6000000\u001B[39m\n\u001B[1;32m      3\u001B[0m nlp\u001B[38;5;241m.\u001B[39mdisable_pipes(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mner\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml4t/lib/python3.8/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml4t/lib/python3.8/site-packages/spacy/util.py:471\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    469\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[0;32m--> 471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models and if you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.max_length = 6000000\n",
    "nlp.disable_pipes('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(articles):\n",
    "    iter_articles = (article for article in articles)\n",
    "    clean_articles = []\n",
    "    for i, doc in enumerate(nlp.pipe(iter_articles, \n",
    "                                     batch_size=100, \n",
    "                                     n_threads=8), 1):\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i / len(articles):.2%}', end=' ', flush=True)\n",
    "        clean_articles.append(clean_doc(doc))\n",
    "    return clean_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_articles = preprocess(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = results_path / 'clean_text'\n",
    "clean_path.write_text('\\n'.join(clean_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = clean_path.read_text().split('\\n')\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_length, token_count = [], Counter()\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    if i % 1e6 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    d = doc.lower().split()\n",
    "    article_length.append(len(d))\n",
    "    token_count.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "(pd.DataFrame(token_count.most_common(), columns=['token', 'count'])\n",
    " .pipe(lambda x: x[~x.token.str.lower().isin(stop_words)])\n",
    " .set_index('token')\n",
    " .squeeze()\n",
    " .iloc[:25]\n",
    " .sort_values()\n",
    " .plot\n",
    " .barh(ax=axes[0], title='Most frequent tokens'))\n",
    "sns.boxenplot(x=pd.Series(article_length), ax=axes[1])\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Word Count (log scale)')\n",
    "axes[1].set_title('Article Length Distribution')\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'fn_explore', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(article_length).describe(percentiles=np.arange(.1, 1.0, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [x.lower() for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set vocab parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = .005\n",
    "max_df = .1\n",
    "ngram_range = (1, 1)\n",
    "binary = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             min_df=min_df,\n",
    "                             max_df=max_df,\n",
    "                             ngram_range=ngram_range,\n",
    "                             binary=binary)\n",
    "dtm = vectorizer.fit_transform(docs)\n",
    "tokens = vectorizer.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = Sparse2Corpus(dtm, documents_columns=False)\n",
    "id2word = pd.Series(tokens).to_dict()\n",
    "dictionary = Dictionary.from_corpus(corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='gensim.log',\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.DEBUG)\n",
    "logging.root.level = logging.DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models with 5-25 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topics in num_topics:\n",
    "    print(topics)\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=topics,\n",
    "                     chunksize=len(docs),\n",
    "                     update_every=1,\n",
    "                     alpha='auto',                     # a-priori belief for the each topics' probability\n",
    "                     eta='auto',                       # a-priori belief on word probability\n",
    "                     decay=0.5,                        # percentage of previous lambda value forgotten\n",
    "                     offset=1.0,\n",
    "                     eval_every=1,\n",
    "                     passes=10,\n",
    "                     iterations=50,\n",
    "                     gamma_threshold=0.001,\n",
    "                     minimum_probability=0.01,         # filter topics with lower probability\n",
    "                     minimum_phi_value=0.01,           # lower bound on term probabilities\n",
    "                     random_state=42)\n",
    "    lda_model.save((results_path / f'model_{topics}').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show results for one model using a vocabulary of 3,800 tokens based on min_df=0.1% and max_df=25% with a single pass to avoid length training time for 20 topics. We can use pyldavis topic_info attribute to compute relevance values for lambda=0.6 that produces the following word list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_lda_model(ntopics, model, corpus=corpus, tokens=tokens):\n",
    "    show_word_list(model=model, corpus=corpus, top=ntopics, save=True)\n",
    "    show_coherence(model=model, corpus=corpus, tokens=tokens, top=ntopics)\n",
    "    vis = prepare(model, corpus, dictionary, mds='tsne')\n",
    "    pyLDAvis.save_html(vis, f'lda_{ntopics}.html')\n",
    "    return 2 ** (-model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_models = {}\n",
    "perplexity ={}\n",
    "for ntopics in num_topics:\n",
    "    print(ntopics)\n",
    "    lda_models[ntopics] = LdaModel.load((results_path / f'model_{ntopics}').as_posix())\n",
    "    perplexity[ntopics] = eval_lda_model(ntopics=ntopics, model=lda_models[ntopics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(perplexity).plot.bar()\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyLDAVis for 15 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = prepare(lda_models[15], corpus, dictionary, mds='tsne')\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAMultiCore Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(results_path / 'lda_multicore_test_results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.num_topics==10].set_index('workers')[['duration', 'test_perplexity']].plot.bar(subplots=True, layout=(1,2), figsize=(14,5), legend=False)\n",
    "sns.despine()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T17:22:36.212849Z",
     "start_time": "2024-05-31T17:22:36.210820Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "name": "conda-env-ml4t-py",
   "language": "python",
   "display_name": "Python [conda env:ml4t] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "_merged",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "387.145px",
    "left": "1219px",
    "right": "1064px",
    "top": "29px",
    "width": "234.312px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
