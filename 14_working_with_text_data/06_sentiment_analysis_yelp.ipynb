{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification and sentiment analysis: Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once text data has been converted into numerical features using the natural language processing techniques discussed in the previous sections, text classification works just like any other classification task.\n",
    "\n",
    "In this notebook, we will apply these preprocessing technique to Yelp business reviews to classify them by review scores and sentiment polarity. More specifically, we will apply sentiment analysis to the significantly larger Yelp business review dataset with five outcome classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:17:17.380987Z",
     "start_time": "2024-05-30T21:17:17.378247Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:17:48.231390Z",
     "start_time": "2024-05-30T21:17:46.561923Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# spacy, textblob and nltk for language processing\n",
    "from textblob import TextBlob\n",
    "\n",
    "# sklearn for feature extraction & modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:17:48.235832Z",
     "start_time": "2024-05-30T21:17:48.233282Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Challenge: business reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [instructions](../data/create_yelp_review_data.ipynb) to create the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:17:48.239469Z",
     "start_time": "2024-05-30T21:17:48.237053Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('..', 'data', 'yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T21:17:48.565458Z",
     "start_time": "2024-05-30T21:17:48.241778Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/yelp/user_reviews.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m yelp_reviews \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muser_reviews.parquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/parquet.py:509\u001B[0m, in \u001B[0;36mread_parquet\u001B[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001B[0m\n\u001B[1;32m    506\u001B[0m     use_nullable_dtypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    507\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[0;32m--> 509\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype_backend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype_backend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/parquet.py:220\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    218\u001B[0m     to_pandas_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit_blocks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m--> 220\u001B[0m path_or_handle, handles, kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43m_get_path_or_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfilesystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    227\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mread_table(\n\u001B[1;32m    228\u001B[0m         path_or_handle, columns\u001B[38;5;241m=\u001B[39mcolumns, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    229\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/parquet.py:110\u001B[0m, in \u001B[0;36m_get_path_or_handle\u001B[0;34m(path, fs, storage_options, mode, is_dir)\u001B[0m\n\u001B[1;32m    100\u001B[0m handles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m fs\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_dir\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;66;03m# fsspec resources can also point to directories\u001B[39;00m\n\u001B[1;32m    109\u001B[0m     \u001B[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001B[39;00m\n\u001B[0;32m--> 110\u001B[0m     handles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m     fs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    114\u001B[0m     path_or_handle \u001B[38;5;241m=\u001B[39m handles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/common.py:868\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    860\u001B[0m             handle,\n\u001B[1;32m    861\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    864\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    865\u001B[0m         )\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    869\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    871\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/yelp/user_reviews.parquet'"
     ]
    }
   ],
   "source": [
    "yelp_reviews = pd.read_parquet(data_dir / 'user_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_dir = Path('results', 'yelp')\n",
    "\n",
    "text_features_dir = yelp_dir / 'data'\n",
    "if not text_features_dir.exists():\n",
    "    text_features_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the number of reviews and the average number of stars per year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews & Stars by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(18, 4))\n",
    "yelp_reviews.year.value_counts().sort_index().plot.bar(title='Reviews per Year', ax=axes[0], rot=0);\n",
    "sns.lineplot(x='year', y='stars', data=yelp_reviews, ax=axes[1])\n",
    "axes[1].set_title('Stars per year')\n",
    "\n",
    "stars_dist = yelp_reviews.stars.value_counts(normalize=True).sort_index().mul(100)\n",
    "stars_dist.index = stars_dist.index.astype(int)\n",
    "stars_dist.plot.barh(title='# Stars Breakdown', ax=axes[2])\n",
    "axes[2].set_xlabel('Share of all Ratings (%)')\n",
    "axes[2].set_ylabel('Number of Stars');\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Years of Membership Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = yelp_reviews.member_yrs.value_counts().div(1000).sort_index().plot.bar(title='Years of Membership',\n",
    "                                                                            rot=0)\n",
    "ax.set_xlabel('Number of Years')\n",
    "ax.set_ylabel(\"Number of Members  ('000)\")\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = yelp_reviews[yelp_reviews.year < 2019].sample(frac=.25)\n",
    "test = yelp_reviews[yelp_reviews.year == 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# Training Obs: {len(train):,.0f} | # Test Obs: {len(test):,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(text_features_dir / 'train.parquet')\n",
    "test.to_parquet(text_features_dir / 'test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del yelp_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(text_features_dir / 'train.parquet')\n",
    "test = pd.read_parquet(text_features_dir / 'test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Yelp review document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2), max_features=10000)\n",
    "train_dtm = vectorizer.fit_transform(train.text)\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(text_features_dir / 'train_dtm', train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dtm = vectorizer.transform(test.text)\n",
    "sparse.save_npz(text_features_dir / 'test_dtm', test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtm = sparse.load_npz(text_features_dir / 'train_dtm.npz')\n",
    "test_dtm = sparse.load_npz(text_features_dir / 'test_dtm.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combine non-text features with the document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains various numerical features. The vectorizers produce [scipy.sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html). To combine the vectorized text data with other features, we need to first convert these to sparse matrices as well; many sklearn objects and other libraries like lightgbm can handle these very memory-efficient data structures. Converting the sparse matrix to a dense numpy array risks memory overflow.\n",
    "\n",
    "Most variables are categorical so we use one-hot encoding since we have a fairly large dataset to accommodate the increase in features.\n",
    "\n",
    "We convert the encoded numerical features and combine them with the document-term matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train.drop(['text', 'stars'], axis=1).assign(source='train'),\n",
    "                test.drop(['text', 'stars'], axis=1).assign(source='test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = df.nunique()\n",
    "binned = pd.concat([(df.loc[:, uniques[uniques > 20].index]\n",
    "                     .apply(pd.qcut, q=10, labels=False, duplicates='drop')),\n",
    "                    df.loc[:, uniques[uniques <= 20].index]], axis=1)\n",
    "binned.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(binned, \n",
    "                         columns=binned.columns.drop('source'), \n",
    "                         drop_first=True)\n",
    "dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummies = dummies[dummies.source=='train'].drop('source', axis=1)\n",
    "train_dummies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Cast other feature columns to float and convert to a sparse matrix.\n",
    "train_numeric = sparse.csr_matrix(train_dummies.astype(np.uint8))\n",
    "train_numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Combine sparse matrices.\n",
    "train_dtm_numeric = sparse.hstack((train_dtm, train_numeric))\n",
    "train_dtm_numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(text_features_dir / 'train_dtm_numeric', \n",
    "                train_dtm_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_dummies = dummies[dummies.source=='test'].drop('source', axis=1)\n",
    "test_numeric = sparse.csr_matrix(test_dummies.astype(np.int8))\n",
    "test_dtm_numeric = sparse.hstack((test_dtm, test_numeric))\n",
    "test_dtm_numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(text_features_dir / 'test_dtm_numeric', test_dtm_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtm_numeric = sparse.load_npz(text_features_dir / 'train_dtm_numeric.npz')\n",
    "test_dtm_numeric = sparse.load_npz(text_features_dir / 'test_dtm_numeric.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, runtime = {}, {}\n",
    "predictions = test[['stars']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the most frequent number of stars (=5) to predict the test set achieve an accuracy close to 51%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_prediction = np.full_like(predictions.stars,\n",
    "                                fill_value=train.stars.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_benchmark = accuracy_score(predictions.stars, naive_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, name, store=False):\n",
    "    start = time()\n",
    "    model.fit(X_train, train.stars)\n",
    "    runtime[name] = time() - start\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    accuracy[result] = accuracy_score(test.stars, predictions[result])\n",
    "    if store:\n",
    "        joblib.dump(model, f'results/{result}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a Naive Bayes classifier using a document-term matrix produced by the CountVectorizer with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 'nb_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(nb, train_dtm, test_dtm, result, store=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction produces 64.4% accuracy on the test set, a 24.2% improvement over the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy[result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = index = list(range(1, 6))\n",
    "pd.DataFrame(confusion_matrix(test.stars, \n",
    "                              predictions[result]),\n",
    "             columns=stars,\n",
    "             index=stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text & Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 'nb_combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(nb, train_dtm_numeric, test_dtm_numeric, result, store=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy[result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression also provides a multinomial training option that is faster and more accurate than the one-vs-all implementation. We use the lbfgs solver (see sklearn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-5, 5, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_text_accuracy = {}\n",
    "log_reg_text_runtime = []\n",
    "for i, C in enumerate(Cs):\n",
    "    start = time()\n",
    "    model = LogisticRegression(C=C,\n",
    "                               multi_class='multinomial',\n",
    "                               solver='lbfgs')\n",
    "\n",
    "    model.fit(train_dtm, train.stars)\n",
    "    log_reg_text_runtime.append(time() - start)\n",
    "    log_reg_text_accuracy[C] = accuracy_score(test.stars,\n",
    "                                              model.predict(test_dtm))\n",
    "\n",
    "    print(f'{C:12.5f}: {log_reg_text_runtime[i]:.2f}s | {log_reg_text_accuracy[C]:.2%}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(log_reg_text_accuracy).to_csv(yelp_dir / 'logreg_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy['lr_text'] = pd.Series(log_reg_text_accuracy).max()\n",
    "runtime['lr_text'] = np.mean(log_reg_text_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_comb_accuracy = {}\n",
    "log_reg_comb_runtime = []\n",
    "for i, C in enumerate(Cs):\n",
    "    start = time()\n",
    "    model = LogisticRegression(C=C,\n",
    "                               multi_class='multinomial',\n",
    "                               solver='lbfgs')\n",
    "\n",
    "    model.fit(train_dtm_numeric, train.stars)\n",
    "    log_reg_comb_runtime.append(time() - start)\n",
    "    log_reg_comb_accuracy[C] = accuracy_score(test.stars,\n",
    "                                              model.predict(test_dtm_numeric))\n",
    "\n",
    "    print(f'{C:12.5f}: {log_reg_comb_runtime[i]:.2f}s | {log_reg_comb_accuracy[C]:.2%}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(log_reg_comb_accuracy).to_csv(yelp_dir / 'logreg_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy['lr_comb'] = pd.Series(log_reg_comb_accuracy).max()\n",
    "runtime['lr_comb'] = np.mean(log_reg_comb_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, we also train a lightgbm Gradient Boosting tree ensemble with default settings and multiclass objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(data=train_dtm_numeric.tocsr().astype(np.float32), \n",
    "                        label=train.stars.sub(1), \n",
    "                        categorical_feature=list(range(train_dtm_numeric.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_test = lgb.Dataset(data=test_dtm_numeric.tocsr().astype(np.float32), \n",
    "                       label=test.stars.sub(1), \n",
    "                       reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective': 'multiclass',\n",
    "         'metrics': ['multi_error'],\n",
    "         'num_class': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "booster = lgb.train(params=param,\n",
    "                    train_set=lgb_train,\n",
    "                    num_boost_round=2000,\n",
    "                    early_stopping_rounds=25,\n",
    "                    valid_sets=[lgb_train, lgb_test],\n",
    "                    verbose_eval=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster.save_model((yelp_dir / 'lgb_model.txt').as_posix());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = booster.predict(test_dtm_numeric.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic settings did not improve over the multinomial logistic regression, but further parameter tuning remains an unused option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy['lgb_comb'] = accuracy_score(test.stars, y_pred_class.argmax(1) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {'nb_combined': 'Naive Bayes',\n",
    "             'lr_comb': 'Logistic Regression',\n",
    "             'lgb_comb': 'LightGBM'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ = {model_map[k]: v for k, v in accuracy.items() if model_map.get(k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_text = pd.read_csv(yelp_dir / 'logreg_text.csv',\n",
    "                       index_col=0,\n",
    "                       squeeze=True)\n",
    "log_reg_combined = pd.read_csv(yelp_dir / 'logreg_combined.csv',\n",
    "                       index_col=0,\n",
    "                       squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4))\n",
    "pd.Series(accuracy_).sort_values().plot.barh(\n",
    "    ax=axes[0], xlim=(.45, .75), title='Accuracy by Model')\n",
    "axes[0].axvline(naive_benchmark, ls='--', lw=1, c='k')\n",
    "\n",
    "\n",
    "log_reg = (log_reg_text.to_frame('text')\n",
    "           .join(log_reg_combined.to_frame('combined')))\n",
    "log_reg.plot(logx=True,\n",
    "             ax=axes[1],\n",
    "             title='Logistic Regression - Model Tuning')\n",
    "axes[1].set_xlabel('Regularization')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "sns.despine()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Textblob for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_review = train.text.sample(1).iloc[0]\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Polarity ranges from -1 (most negative) to 1 (most positive).\n",
    "TextBlob(sample_review).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns the polarity.\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment'] = train.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = train[['stars', 'text']].sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame column for sentiment (Warning: SLOW!).\n",
    "sample_reviews['sentiment'] = sample_reviews.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Box plot of sentiment grouped by stars\n",
    "sns.boxenplot(x='stars', y='sentiment', data=train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Widen the column display.\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Reviews with most negative sentiment\n",
    "train[train.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Negative sentiment in a 5-star review\n",
    "train.loc[(train.stars == 5) & (train.sentiment < -0.3), 'text'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Positive sentiment in a 1-star review\n",
    "train.loc[(train.stars == 1) & (train.sentiment > 0.5), 'text'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the column display width.\n",
    "pd.reset_option('max_colwidth')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "name": "conda-env-ml4t-py",
   "language": "python",
   "display_name": "Python [conda env:ml4t] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
