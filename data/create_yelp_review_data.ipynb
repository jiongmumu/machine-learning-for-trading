{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Yelp Reviews data for Sentiment Analysis and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:01:23.428236Z",
     "start_time": "2024-05-31T15:01:23.399368Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:01:23.435090Z",
     "start_time": "2024-05-31T15:01:23.431411Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of several files with information on the business, the user, the review and other aspects that Yelp provides to encourage data science innovation.\n",
    "\n",
    "The data consists of several files with information on the business, the user, the review and other aspects that Yelp provides to encourage data science innovation. \n",
    "\n",
    "We will use around six million reviews produced over the 2010-2019 period to extract text features. In addition, we will use other information submitted with the review about the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the data from [here](https://www.yelp.com/dataset) in json format after accepting the license. The 2020 version has 4.7GB (compressed) and around 10.5GB (uncompressed) of text data.\n",
    "\n",
    "After download, extract the following two of the five `.json` files into to `./yelp/json`:\n",
    "- the `yelp_academic_dataset_user.json`\n",
    "- the `yelp_academic_dataset_reviews.json`\n",
    "\n",
    "Rename both files by stripping out the `yelp_academic_dataset_` prefix so you have the following directory structure:\n",
    "```\n",
    "data\n",
    "|-create_yelp_review_data.ipynb\n",
    "|-yelp\n",
    "    |-json\n",
    "        |-user.json\n",
    "        |-review.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:23:35.702622Z",
     "start_time": "2024-05-31T15:23:35.699287Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_dir = Path('yelp')\n",
    "\n",
    "if not yelp_dir.exists():\n",
    "    yelp_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse json and store as parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert json to faster parquet format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:26:27.977456Z",
     "start_time": "2024-05-31T15:23:54.673839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m data \u001B[38;5;241m=\u001B[39m json_file\u001B[38;5;241m.\u001B[39mread_text(encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     11\u001B[0m json_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([l\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m     12\u001B[0m                             \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m data\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m l\u001B[38;5;241m.\u001B[39mstrip()]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 13\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241m.\u001B[39mloads(json_data)\n\u001B[1;32m     14\u001B[0m df \u001B[38;5;241m=\u001B[39m json_normalize(data)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreview\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "for fname in ['review', 'user']:\n",
    "    print(fname)\n",
    "    \n",
    "    json_file = yelp_dir / 'json' / f'{fname}.json'\n",
    "    parquet_file = yelp_dir / f'{fname}.parquet'\n",
    "    if parquet_file.exists():\n",
    "        print('\\talready exists')\n",
    "        continue\n",
    "\n",
    "    data = json_file.read_text(encoding='utf-8')\n",
    "    json_data = '[' + ','.join([l.strip()\n",
    "                                for l in data.split('\\n') if l.strip()]) + ']\\n'\n",
    "    data = json.loads(json_data)\n",
    "    df = json_normalize(data)\n",
    "    if fname == 'review':\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        latest = df.date.max()\n",
    "        df['year'] = df.date.dt.year\n",
    "        df['month'] = df.date.dt.month\n",
    "        df = df.drop(['date', 'business_id', 'review_id'], axis=1)\n",
    "    if fname == 'user':\n",
    "        df.yelping_since = pd.to_datetime(df.yelping_since)\n",
    "        df = (df.assign(member_yrs=lambda x: (latest - x.yelping_since)\n",
    "                        .dt.days.div(365).astype(int))\n",
    "              .drop(['elite', 'friends', 'name', 'yelping_since'], axis=1))\n",
    "    df.dropna(how='all', axis=1).to_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can remove the json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T15:01:23.545995Z",
     "start_time": "2024-05-31T15:01:23.545881Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_files(remove=False):\n",
    "    combined_file = yelp_dir / 'user_reviews.parquet'\n",
    "    if not combined_file.exists():\n",
    "        user = pd.read_parquet(yelp_dir / 'user.parquet')\n",
    "        print(user.info(null_counts=True))\n",
    "\n",
    "        review = pd.read_parquet(yelp_dir / 'review.parquet')\n",
    "        print(review.info(null_counts=True))\n",
    "\n",
    "        combined = (review.merge(user, on='user_id',\n",
    "                                 how='left', suffixes=['', '_user'])\n",
    "                    .drop('user_id', axis=1))\n",
    "        combined = combined[combined.stars > 0]\n",
    "        print(combined.info(null_counts=True))\n",
    "        combined.to_parquet(yelp_dir / 'user_reviews.parquet')\n",
    "    else:\n",
    "        print('already merged')\n",
    "    if remove:\n",
    "        for fname in ['user', 'review']:\n",
    "            f = yelp_dir / (fname + '.parquet')\n",
    "            if f.exists():\n",
    "                f.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_files(remove=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4t] *",
   "language": "python",
   "name": "conda-env-ml4t-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
